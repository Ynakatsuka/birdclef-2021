# @package _group_
skip_training: False

train:
  batch_size: 32
  backbone_lr_ratio:
  encoder_lr_ratio:

evaluation:
  batch_size: 64
  save_prediction: True
  dirpath: '${save_dir}/predictions/oof/'
  name: '${experiment_name}_fold_${dataset.dataset.params.idx_fold}.npy'

trainer:
  resume_from_checkpoint: # '${trainer.dir}/trainer_exp000_aug_exp000_fold_0_epoch=009.ckpt'
  max_epochs: 20
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 1
  # amp_backend: 'native'
  # amp_level: 'O2'
  precision: 16
  gpus: [0, 1]
  accelerator: 'ddp'
  sync_batchnorm: True
  auto_lr_find: False
  auto_scale_batch_size: False
  benchmark: True
  deterministic: True
  num_sanity_val_steps: 0

inference:
  input_dir: '${input_dir}/train_soundscapes/'
  dirpath: '${save_dir}/predictions/'
  filename: '${experiment_name}_fold_${dataset.dataset.params.idx_fold}.pkl'
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: ${project}
  name: ${experiment_name}
  # mode: 'disabled'

callbacks:
  ModelCheckpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: 'val_loss'
    save_last: False
    save_top_k: 1
    dirpath: '${save_dir}/models/${experiment_name}'
    filename: 'fold_${dataset.dataset.params.idx_fold}_{epoch:03d}'

model:
  name: 'resnet34'
  params:
    num_classes: 397
    pretrained: True
  last_linear:
    replace: True
    dropout: 0.5
    pool_type: 
    use_seblock: False 
    training: False # freezing pretrained weights and training last linear only
    params: 
      max_epochs: 4
      check_val_every_n_epoch: 3

loss:
  name: 'BCEWithLogitsLoss'

metrics: 
  multilabel_auroc:
    name: 'multilabel_auroc'
    params:
      num_classes: num_classes

optimizer:
  name: 'RAdam'
  params:
    lr: 0.001
    weight_decay: 0.01

scheduler:
  name: 'OneCycleLR'
  params:
    epochs: 2
    max_lr: 0.00025
    pct_start: 0.033
    steps_per_epoch: 568

lightning_module: 
  name: 'LightningModuleBase'
  params:
      max_epochs: ${trainer.trainer.max_epochs}
      disable_strong_transform_in_last_epochs: 5    

hooks:
  post_forward:
